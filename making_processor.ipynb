{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "making_processor.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyO0Ion1FGgkKT7q480ryU2r",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Samreenhabib/Urdu-OCR/blob/main/making_processor.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q transformers"
      ],
      "metadata": {
        "id": "SGZhIBek_N2s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from transformers import TrOCRProcessor\n",
        "\n",
        "# processor = TrOCRProcessor.from_pretrained(\"microsoft/trocr-base-handwritten\")\n",
        "# processor.save_pretrained('./ms-processor')"
      ],
      "metadata": {
        "id": "QeZoJcmy6zwe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_rmekvjh-omy"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoFeatureExtractor, AutoTokenizer\n",
        "from transformers import TrOCRProcessor, VisionEncoderDecoderModel\n",
        "feature_extractor = AutoFeatureExtractor.from_pretrained(\"google/vit-base-patch16-384\")\n",
        "decoder_tokenizer = AutoTokenizer.from_pretrained(\"urduhack/roberta-urdu-small\")\n",
        "processor =TrOCRProcessor(feature_extractor=feature_extractor, tokenizer=decoder_tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "processor.save_pretrained('./processor')"
      ],
      "metadata": {
        "id": "PYBr94va_jv0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q datasets jiwer"
      ],
      "metadata": {
        "id": "tJ2BlTX641SV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import cv2\n",
        "df = pd.read_excel('/content/test.xlsx', header=None)\n",
        "df.rename(columns={0: \"file_name\", 1: \"text\"}, inplace=True)\n",
        "df['text']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KfqpUIo15AS4",
        "outputId": "f521f9a8-0aa8-46a0-b0cd-31a2c24a1a8b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0                         ایک پاکستانی نوجوان کی داستان\n",
              "1                  ایک منفرد انداز میں لکھا گیا سفرنام٘\n",
              "2                                 قارئین کے لَے تحف خاص\n",
              "3                                رضوان علی گھمن (جرمنی)\n",
              "4     خدا نے اس پوری کَاءنات کو بنایا ہے- خدا کی بنا...\n",
              "5     منفرد ہے- جب آپ نظر اٹھا کر آسمان کی طرف دیکھت...\n",
              "6     عظمت کی جھلک نظر آتی ہے- سورج چاند ستاروں اور ...\n",
              "7     صرف پانچ چھ فٹ کا ہوتا ہے لیکن یِہی انسان جب م...\n",
              "8     میری یہ کہانی بھی عشق کی راہوں میں فنا ہونے وا...\n",
              "9     معصوم جسم جب ملنے پر آتے ہیں تو اس آگ میں جلنے...\n",
              "10                                        ہی کا کمال ہے\n",
              "Name: text, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train_df, test_df = train_test_split(df, test_size=0.3)\n",
        "# we reset the indices to start from zero\n",
        "train_df.reset_index(drop=True, inplace=True)\n",
        "test_df.reset_index(drop=True, inplace=True)"
      ],
      "metadata": {
        "id": "L4sMaHuQ5gTV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from PIL import Image\n",
        "\n",
        "class IAMDataset(Dataset):\n",
        "    def __init__(self, root_dir, df, processor, max_target_length=128):\n",
        "        self.root_dir = root_dir\n",
        "        self.df = df\n",
        "        self.processor = processor\n",
        "        self.max_target_length = max_target_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # get file name + text \n",
        "        file_name = self.df['file_name'][idx]\n",
        "        text = self.df['text'][idx]\n",
        "        # prepare image (i.e. resize + normalize)\n",
        "        image = Image.open(self.root_dir + file_name).convert(\"RGB\")\n",
        "        pixel_values = self.processor(image, return_tensors=\"pt\").pixel_values\n",
        "        # add labels (input_ids) by encoding the text\n",
        "        labels = self.processor.tokenizer(text, padding=\"max_length\",max_length=self.max_target_length).input_ids\n",
        "        # important: make sure that PAD tokens are ignored by the loss function\n",
        "        labels = [label if label != self.processor.tokenizer.pad_token_id else -100 for label in labels]\n",
        "\n",
        "        encoding = {\"pixel_values\": pixel_values.squeeze(), \"labels\": torch.tensor(labels)}\n",
        "        return encoding"
      ],
      "metadata": {
        "id": "6hyna5wS5hX4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrOCRProcessor\n",
        "\n",
        "processor = TrOCRProcessor.from_pretrained(\"./processor\")\n",
        "train_dataset = IAMDataset(root_dir='/content/image/',\n",
        "                           df=train_df,\n",
        "                           processor=processor)\n",
        "eval_dataset = IAMDataset(root_dir='/content/image/',\n",
        "                           df=test_df,\n",
        "                           processor=processor)"
      ],
      "metadata": {
        "id": "DMa6KXH15wsx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Number of training examples:\", len(train_dataset))\n",
        "print(\"Number of validation examples:\", len(eval_dataset))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zS0V-GTS6YkT",
        "outputId": "295fe667-b17a-498e-e49c-12df1bf6ade1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of training examples: 7\n",
            "Number of validation examples: 4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "encoding = train_dataset[3]\n",
        "for k,v in encoding.items():\n",
        "  print(k, v.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c69U-3YH6cMF",
        "outputId": "7548d140-bb31-4d81-a6a3-dd06b45d70d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "pixel_values torch.Size([3, 384, 384])\n",
            "labels torch.Size([128])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "image = Image.open(train_dataset.root_dir + train_df['file_name'][3]).convert(\"RGB\")\n",
        "image"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 63
        },
        "id": "tRgMIlp06dA-",
        "outputId": "dde67c5a-8005-4d4a-f794-5c21974faab0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<PIL.Image.Image image mode=RGB size=1253x49 at 0x7F2413779510>"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "labels = encoding['labels']\n",
        "labels[labels == -100] = processor.tokenizer.pad_token_id\n",
        "label_str = processor.decode(labels, skip_special_tokens=True)\n",
        "print(label_str)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "viLah64h6sW1",
        "outputId": "857a2e73-8b1e-442e-f99d-4a268ffd9995"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "خدا نے اس پوری کَاءنات کو بنایا ہے- خدا کی بنایَ ہویَ اس پوری کانَات میں صرف انسان ہی وہ چیز ہے جو \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import VisionEncoderDecoderModel\n",
        "\n",
        "\n",
        "model = VisionEncoderDecoderModel.from_encoder_decoder_pretrained(\n",
        "    \"google/vit-base-patch16-384\", \"urduhack/roberta-urdu-small\"\n",
        ")\n",
        "# set decoder config to causal lm\n",
        "model.config.decoder.is_decoder = True\n",
        "model.config.decoder.add_cross_attention = True\n",
        "# set special tokens used for creating the decoder_input_ids from the labels\n",
        "model.config.decoder_start_token_id = processor.tokenizer.cls_token_id\n",
        "model.config.pad_token_id = processor.tokenizer.pad_token_id\n",
        "# make sure vocab size is set correctly\n",
        "model.config.vocab_size = model.config.decoder.vocab_size\n",
        "\n",
        "# set beam search parameters\n",
        "model.config.eos_token_id = processor.tokenizer.sep_token_id\n",
        "model.config.max_length = 64\n",
        "model.config.early_stopping = True\n",
        "model.config.no_repeat_ngram_size = 3\n",
        "model.config.length_penalty = 2.0\n",
        "model.config.num_beams = 4"
      ],
      "metadata": {
        "id": "vrJ3ojeJA6n9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "using steps "
      ],
      "metadata": {
        "id": "I-_bwRqJFvDH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
        "\n",
        "training_args = Seq2SeqTrainingArguments(\n",
        "    predict_with_generate=True,\n",
        "    evaluation_strategy=\"steps\",\n",
        "    per_device_train_batch_size=2,\n",
        "    per_device_eval_batch_size=2,\n",
        "    fp16=True, \n",
        "    output_dir=\"./\",\n",
        "    logging_steps=2,\n",
        "    save_steps=1000,\n",
        "    eval_steps=200,\n",
        "    num_train_epochs=6,  \n",
        ")"
      ],
      "metadata": {
        "id": "_otg0vO8BOU3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "evaluating using epoch"
      ],
      "metadata": {
        "id": "Xz9lc08aF7pr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "training_args = Seq2SeqTrainingArguments(\n",
        "   evaluation_strategy = \"epoch\",\n",
        "   learning_rate=2e-4,\n",
        "   per_device_train_batch_size=2,\n",
        "   per_device_eval_batch_size=2,\n",
        "   weight_decay=0.01,\n",
        "   save_total_limit=3,\n",
        "   num_train_epochs=6,\n",
        "   output_dir=\"./\",\n",
        "   predict_with_generate=True,  \n",
        ")"
      ],
      "metadata": {
        "id": "1-AHdsYMFe__"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_metric\n",
        "cer_metric = load_metric(\"cer\")"
      ],
      "metadata": {
        "id": "-UNPv-KiBtvp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_metrics(pred):\n",
        "    labels_ids = pred.label_ids\n",
        "    pred_ids = pred.predictions\n",
        "\n",
        "    pred_str = processor.batch_decode(pred_ids, skip_special_tokens=True)\n",
        "    labels_ids[labels_ids == -100] = processor.tokenizer.pad_token_id\n",
        "    label_str = processor.batch_decode(labels_ids, skip_special_tokens=True)\n",
        "\n",
        "    cer = cer_metric.compute(predictions=pred_str, references=label_str)\n",
        "\n",
        "    return {\"cer\": cer}"
      ],
      "metadata": {
        "id": "vOYddNXnBxr2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import default_data_collator\n",
        "\n",
        "# instantiate trainer\n",
        "trainer = Seq2SeqTrainer(\n",
        "    model=model,\n",
        "    tokenizer=processor.feature_extractor,\n",
        "    args=training_args,\n",
        "    compute_metrics=compute_metrics,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=eval_dataset,\n",
        "    data_collator=default_data_collator\n",
        ")\n",
        "trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 857
        },
        "id": "8k_Ufd6HB1Px",
        "outputId": "b353a36c-4a7f-4ac5-fc74-adc684c27e9f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n",
            "***** Running training *****\n",
            "  Num examples = 7\n",
            "  Num Epochs = 6\n",
            "  Instantaneous batch size per device = 2\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 2\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 24\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='24' max='24' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [24/24 00:39, Epoch 6/6]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Cer</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>No log</td>\n",
              "      <td>9.447093</td>\n",
              "      <td>2.709877</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>No log</td>\n",
              "      <td>7.128946</td>\n",
              "      <td>1.370370</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>No log</td>\n",
              "      <td>7.455116</td>\n",
              "      <td>1.401235</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>No log</td>\n",
              "      <td>7.845613</td>\n",
              "      <td>1.783951</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>No log</td>\n",
              "      <td>8.057783</td>\n",
              "      <td>2.043210</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>No log</td>\n",
              "      <td>8.179310</td>\n",
              "      <td>2.043210</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 4\n",
            "  Batch size = 2\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 4\n",
            "  Batch size = 2\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 4\n",
            "  Batch size = 2\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 4\n",
            "  Batch size = 2\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 4\n",
            "  Batch size = 2\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 4\n",
            "  Batch size = 2\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=24, training_loss=0.9400688012441, metrics={'train_runtime': 40.4242, 'train_samples_per_second': 1.039, 'train_steps_per_second': 0.594, 'total_flos': 2.2378738933039104e+16, 'train_loss': 0.9400688012441, 'epoch': 6.0})"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.save_model('./trainer')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D3J3XGY6GfkD",
        "outputId": "222ddded-d50a-4665-a17d-e368098cd42e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Saving model checkpoint to ./trainer\n",
            "Configuration saved in ./trainer/config.json\n",
            "Model weights saved in ./trainer/pytorch_model.bin\n",
            "Feature extractor saved in ./trainer/preprocessor_config.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = VisionEncoderDecoderModel.from_pretrained(\"./trainer\")"
      ],
      "metadata": {
        "id": "Jzfohl8wG2J2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image = Image.open('/content/6.png').convert(\"RGB\")"
      ],
      "metadata": {
        "id": "okXisArpHLzQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "VQnwWb5LPngA",
        "outputId": "ad2f69b3-90e2-4e4b-c1ac-69bffb78fc09"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<PIL.Image.Image image mode=RGB size=1253x94 at 0x7F23A389D210>"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pixel_values = processor.feature_extractor(image, return_tensors=\"pt\").pixel_values \n",
        "print(pixel_values.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A5MR7dxhHR7W",
        "outputId": "7f68d1ae-71e9-425f-cac1-a427faaba681"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 3, 384, 384])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generated_ids = model.generate(pixel_values)\n",
        "generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
        "print(generated_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2e4ruhWiHp4-",
        "outputId": "d380e4cd-9e85-4285-8ae1-51a9a682acd7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "منفرد ہے- جب آپ نظر اٹھا کر آسمان کی طرف دیکھتے ہیں تو آپ کو کاءنات کی اس لامحدود وسعتوں میں خدا کی\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test = Image.open('/content/8.png').convert(\"RGB\")\n",
        "testvalues = processor.feature_extractor(test, return_tensors=\"pt\").pixel_values \n",
        "print(testvalues.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QHTWc4xjd3_b",
        "outputId": "38f9d4a8-2f5b-4f53-8c42-c093c3cad65e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 3, 384, 384])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "testids = model.generate(testvalues)\n",
        "testtext = processor.batch_decode(testids, skip_special_tokens=True)[0]\n",
        "print(testtext)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-pHS_LzceBgO",
        "outputId": "e422ed6d-cc96-49b0-9d3c-2582faf0e03e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "منفرد ہے- جب آپ نظر اٹھا کر آسمان کی طرف دیکھتے ہیں تو آپ کو کاءنات کی اس لامحدود وسعتوں میں خدا کی\n"
          ]
        }
      ]
    }
  ]
}