{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "SGZhIBek_N2s"
   },
   "outputs": [],
   "source": [
    "pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!pip install torchvision "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QeZoJcmy6zwe"
   },
   "outputs": [],
   "source": [
    "# from transformers import TrOCRProcessor\n",
    "\n",
    "# processor = TrOCRProcessor.from_pretrained(\"microsoft/trocr-base-handwritten\")\n",
    "# processor.save_pretrained('./ms-processor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "_rmekvjh-omy"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoFeatureExtractor, AutoTokenizer\n",
    "from transformers import TrOCRProcessor, VisionEncoderDecoderModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feature_extractor = AutoFeatureExtractor.from_pretrained(\"google/vit-base-patch16-384\")\n",
    "decoder_tokenizer = AutoTokenizer.from_pretrained(\"urduhack/roberta-urdu-small\")\n",
    "processor =TrOCRProcessor(feature_extractor=feature_extractor, tokenizer=decoder_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "PYBr94va_jv0"
   },
   "outputs": [],
   "source": [
    "processor.save_pretrained('./processor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "tJ2BlTX641SV"
   },
   "outputs": [],
   "source": [
    "!pip install -q datasets jiwer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KfqpUIo15AS4",
    "outputId": "394b597c-b167-45e0-eaec-1b20e73d3169"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_name</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.png</td>\n",
       "      <td>پشاور،بنوں (نمائندہ جنگ،اے ایف پی) بنوں میں اق...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.png</td>\n",
       "      <td>اسکے ساتھ ملحقہ علاقے کے عوام نے بجلی و گیس</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.png</td>\n",
       "      <td>کی لوڈشیڈنگ کیخلاف مظاہرہ کیا۔ پولیس تشدد سے ا...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.png</td>\n",
       "      <td>منشور علی جاں بحق اور 2خواتین سمیت 14 افرادزخم...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.png</td>\n",
       "      <td>علاقے میں کرفیو نافذ کردیا گیا ہے۔ مکینوں نے ا...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10058</th>\n",
       "      <td>10058.png</td>\n",
       "      <td>جبکہ حکومت ان متاثرین کی امداد اور دوبارہ بحال...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10059</th>\n",
       "      <td>10059.png</td>\n",
       "      <td>شب و روز کوشاں ہے۔ انہوں نے کہا کہ نقصانات</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10060</th>\n",
       "      <td>10060.png</td>\n",
       "      <td>کے سروے کا کام جاری ہے۔ دونوں رہنماؤں نے اس</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10061</th>\n",
       "      <td>10061.png</td>\n",
       "      <td>عزم کا اعادہ کیا کہ صوبے کے متاثرہ افراد کی</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10062</th>\n",
       "      <td>10062.png</td>\n",
       "      <td>امداد کے لئے بھرپور کوششیں کی جائیں گی۔</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10063 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       file_name                                               text\n",
       "0          0.png  پشاور،بنوں (نمائندہ جنگ،اے ایف پی) بنوں میں اق...\n",
       "1          1.png        اسکے ساتھ ملحقہ علاقے کے عوام نے بجلی و گیس\n",
       "2          2.png  کی لوڈشیڈنگ کیخلاف مظاہرہ کیا۔ پولیس تشدد سے ا...\n",
       "3          3.png  منشور علی جاں بحق اور 2خواتین سمیت 14 افرادزخم...\n",
       "4          4.png  علاقے میں کرفیو نافذ کردیا گیا ہے۔ مکینوں نے ا...\n",
       "...          ...                                                ...\n",
       "10058  10058.png  جبکہ حکومت ان متاثرین کی امداد اور دوبارہ بحال...\n",
       "10059  10059.png         شب و روز کوشاں ہے۔ انہوں نے کہا کہ نقصانات\n",
       "10060  10060.png        کے سروے کا کام جاری ہے۔ دونوں رہنماؤں نے اس\n",
       "10061  10061.png        عزم کا اعادہ کیا کہ صوبے کے متاثرہ افراد کی\n",
       "10062  10062.png           امداد کے لئے بھرپور کوششیں کی جائیں گی۔ \n",
       "\n",
       "[10063 rows x 2 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import cv2\n",
    "df = pd.read_csv('all_data/train/labels.csv', header=None)\n",
    "df.rename(columns={0: \"file_name\", 1: \"text\"}, inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "L4sMaHuQ5gTV"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_df, test_df = train_test_split(df, test_size=0.3)\n",
    "# we reset the indices to start from zero\n",
    "train_df.reset_index(drop=True, inplace=True)\n",
    "test_df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "6hyna5wS5hX4"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "\n",
    "class IAMDataset(Dataset):\n",
    "    def __init__(self, root_dir, df, processor, max_target_length=128):\n",
    "        self.root_dir = root_dir\n",
    "        self.df = df\n",
    "        self.processor = processor\n",
    "        self.max_target_length = max_target_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # get file name + text \n",
    "        file_name = self.df['file_name'][idx]\n",
    "        text = self.df['text'][idx]\n",
    "        # prepare image (i.e. resize + normalize)\n",
    "        image = Image.open(self.root_dir + file_name).convert(\"RGB\")\n",
    "        pixel_values = self.processor(image, return_tensors=\"pt\").pixel_values\n",
    "        # add labels (input_ids) by encoding the text\n",
    "        labels = self.processor.tokenizer(text, padding=\"max_length\",max_length=self.max_target_length).input_ids\n",
    "        # important: make sure that PAD tokens are ignored by the loss function\n",
    "        labels = [label if label != self.processor.tokenizer.pad_token_id else -100 for label in labels]\n",
    "\n",
    "        encoding = {\"pixel_values\": pixel_values.squeeze(), \"labels\": torch.tensor(labels)}\n",
    "        return encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "DMa6KXH15wsx"
   },
   "outputs": [],
   "source": [
    "from transformers import TrOCRProcessor\n",
    "\n",
    "processor = TrOCRProcessor.from_pretrained(\"./processor\")\n",
    "train_dataset = IAMDataset(root_dir='./all_data/train/',\n",
    "                           df=train_df,\n",
    "                           processor=processor)\n",
    "eval_dataset = IAMDataset(root_dir='./all_data/train/',\n",
    "                           df=test_df,\n",
    "                           processor=processor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zS0V-GTS6YkT",
    "outputId": "4c49308a-f455-4b9f-82aa-1d878a157d52"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 7044\n",
      "Number of validation examples: 3019\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of training examples:\", len(train_dataset))\n",
    "print(\"Number of validation examples:\", len(eval_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c69U-3YH6cMF",
    "outputId": "28b3e6b6-412a-4425-d8e2-706a812f8a4e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pixel_values torch.Size([3, 384, 384])\n",
      "labels torch.Size([128])\n"
     ]
    }
   ],
   "source": [
    "encoding = train_dataset[3]\n",
    "for k,v in encoding.items():\n",
    "    print(k, v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 62
    },
    "id": "tRgMIlp06dA-",
    "outputId": "980adc8c-2305-4a10-fd19-644cd4ffc502"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=783x95 at 0x1E5D1883FA0>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image = Image.open(train_dataset.root_dir + train_df['file_name'][5]).convert(\"RGB\")\n",
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "viLah64h6sW1",
    "outputId": "407e40f0-b61a-41fb-b691-03c6b9ae87d9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "تھی۔ کامران خان نے بتایا کہ ذوالفقار مرزا نے اجلاس\n"
     ]
    }
   ],
   "source": [
    "labels = encoding['labels']\n",
    "labels[labels == -100] = processor.tokenizer.pad_token_id\n",
    "label_str = processor.decode(labels, skip_special_tokens=True)\n",
    "print(label_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "vrJ3ojeJA6n9"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8862521690f9484794b833cf89a7418d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/348M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google/vit-base-patch16-384 were not used when initializing ViTModel: ['classifier.weight', 'classifier.bias']\n",
      "- This IS expected if you are initializing ViTModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ViTModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-384 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85a2ddf1db4345c19d22bbe81635c6c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/507M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForCausalLM were not initialized from the model checkpoint at urduhack/roberta-urdu-small and are newly initialized: ['roberta.encoder.layer.11.crossattention.self.key.weight', 'roberta.encoder.layer.11.crossattention.self.value.bias', 'roberta.encoder.layer.1.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.3.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.6.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.4.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.10.crossattention.output.dense.weight', 'roberta.encoder.layer.5.crossattention.self.query.weight', 'roberta.encoder.layer.8.crossattention.self.value.weight', 'roberta.encoder.layer.4.crossattention.self.query.weight', 'roberta.encoder.layer.11.crossattention.self.query.weight', 'roberta.encoder.layer.11.crossattention.output.dense.weight', 'roberta.encoder.layer.9.crossattention.self.value.bias', 'roberta.encoder.layer.6.crossattention.self.query.weight', 'roberta.encoder.layer.4.crossattention.self.value.bias', 'roberta.encoder.layer.8.crossattention.output.dense.weight', 'roberta.encoder.layer.4.crossattention.self.key.weight', 'roberta.encoder.layer.6.crossattention.self.key.weight', 'roberta.encoder.layer.3.crossattention.self.key.weight', 'roberta.encoder.layer.2.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.3.crossattention.self.value.weight', 'roberta.encoder.layer.6.crossattention.output.dense.bias', 'roberta.encoder.layer.6.crossattention.output.dense.weight', 'roberta.encoder.layer.9.crossattention.self.value.weight', 'roberta.encoder.layer.7.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.1.crossattention.output.dense.weight', 'roberta.encoder.layer.10.crossattention.output.dense.bias', 'roberta.encoder.layer.10.crossattention.self.query.weight', 'roberta.encoder.layer.9.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.9.crossattention.output.dense.weight', 'roberta.encoder.layer.8.crossattention.self.key.weight', 'roberta.encoder.layer.8.crossattention.output.dense.bias', 'roberta.encoder.layer.8.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.5.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.1.crossattention.self.key.weight', 'roberta.encoder.layer.10.crossattention.self.value.bias', 'roberta.encoder.layer.8.crossattention.self.value.bias', 'roberta.encoder.layer.3.crossattention.self.value.bias', 'roberta.encoder.layer.4.crossattention.self.value.weight', 'roberta.encoder.layer.0.crossattention.self.value.bias', 'roberta.encoder.layer.7.crossattention.self.value.weight', 'roberta.encoder.layer.9.crossattention.self.query.bias', 'roberta.encoder.layer.2.crossattention.self.key.bias', 'roberta.encoder.layer.5.crossattention.output.dense.bias', 'roberta.encoder.layer.5.crossattention.self.value.weight', 'roberta.encoder.layer.9.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.11.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.2.crossattention.self.value.bias', 'roberta.encoder.layer.7.crossattention.output.dense.bias', 'roberta.encoder.layer.10.crossattention.self.query.bias', 'roberta.encoder.layer.1.crossattention.self.value.weight', 'roberta.encoder.layer.9.crossattention.self.query.weight', 'roberta.encoder.layer.11.crossattention.output.dense.bias', 'roberta.encoder.layer.0.crossattention.output.dense.weight', 'roberta.encoder.layer.2.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.3.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.6.crossattention.self.query.bias', 'roberta.encoder.layer.1.crossattention.self.query.weight', 'roberta.encoder.layer.0.crossattention.self.key.weight', 'roberta.encoder.layer.6.crossattention.self.value.bias', 'roberta.encoder.layer.7.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.11.crossattention.self.value.weight', 'roberta.encoder.layer.0.crossattention.self.value.weight', 'roberta.encoder.layer.0.crossattention.output.dense.bias', 'roberta.encoder.layer.7.crossattention.self.key.bias', 'roberta.encoder.layer.10.crossattention.self.key.bias', 'roberta.encoder.layer.2.crossattention.self.key.weight', 'roberta.encoder.layer.0.crossattention.self.key.bias', 'roberta.encoder.layer.10.crossattention.self.key.weight', 'roberta.encoder.layer.5.crossattention.self.value.bias', 'roberta.encoder.layer.7.crossattention.output.dense.weight', 'roberta.encoder.layer.1.crossattention.self.query.bias', 'roberta.encoder.layer.10.crossattention.self.value.weight', 'roberta.encoder.layer.6.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.4.crossattention.output.dense.bias', 'roberta.encoder.layer.0.crossattention.self.query.weight', 'roberta.encoder.layer.0.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.4.crossattention.self.query.bias', 'roberta.encoder.layer.6.crossattention.self.key.bias', 'roberta.encoder.layer.1.crossattention.self.key.bias', 'roberta.encoder.layer.4.crossattention.self.key.bias', 'roberta.encoder.layer.4.crossattention.output.dense.weight', 'roberta.encoder.layer.9.crossattention.self.key.weight', 'roberta.encoder.layer.3.crossattention.output.dense.bias', 'roberta.encoder.layer.11.crossattention.self.key.bias', 'roberta.encoder.layer.3.crossattention.self.key.bias', 'roberta.encoder.layer.7.crossattention.self.query.weight', 'roberta.encoder.layer.7.crossattention.self.value.bias', 'roberta.encoder.layer.4.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.3.crossattention.self.query.weight', 'roberta.encoder.layer.2.crossattention.output.dense.weight', 'roberta.encoder.layer.5.crossattention.output.dense.weight', 'roberta.encoder.layer.8.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.10.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.8.crossattention.self.query.weight', 'roberta.encoder.layer.3.crossattention.output.dense.weight', 'roberta.encoder.layer.8.crossattention.self.query.bias', 'roberta.encoder.layer.9.crossattention.output.dense.bias', 'roberta.encoder.layer.9.crossattention.self.key.bias', 'roberta.encoder.layer.1.crossattention.output.dense.bias', 'roberta.encoder.layer.1.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.0.crossattention.self.query.bias', 'roberta.encoder.layer.11.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.0.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.5.crossattention.self.query.bias', 'roberta.encoder.layer.11.crossattention.self.query.bias', 'roberta.encoder.layer.1.crossattention.self.value.bias', 'roberta.encoder.layer.8.crossattention.self.key.bias', 'roberta.encoder.layer.2.crossattention.self.query.bias', 'roberta.encoder.layer.5.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.2.crossattention.self.value.weight', 'roberta.encoder.layer.10.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.7.crossattention.self.query.bias', 'roberta.encoder.layer.3.crossattention.self.query.bias', 'roberta.encoder.layer.2.crossattention.self.query.weight', 'roberta.encoder.layer.7.crossattention.self.key.weight', 'roberta.encoder.layer.5.crossattention.self.key.bias', 'roberta.encoder.layer.6.crossattention.self.value.weight', 'roberta.encoder.layer.5.crossattention.self.key.weight', 'roberta.encoder.layer.2.crossattention.output.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import VisionEncoderDecoderModel\n",
    "\n",
    "\n",
    "model = VisionEncoderDecoderModel.from_encoder_decoder_pretrained(\n",
    "    \"google/vit-base-patch16-384\", \"urduhack/roberta-urdu-small\"\n",
    ")\n",
    "# set decoder config to causal lm\n",
    "model.config.decoder.is_decoder = True\n",
    "model.config.decoder.add_cross_attention = True\n",
    "# set special tokens used for creating the decoder_input_ids from the labels\n",
    "model.config.decoder_start_token_id = processor.tokenizer.cls_token_id\n",
    "model.config.pad_token_id = processor.tokenizer.pad_token_id\n",
    "# make sure vocab size is set correctly\n",
    "model.config.vocab_size = model.config.decoder.vocab_size\n",
    "\n",
    "# set beam search parameters\n",
    "model.config.eos_token_id = processor.tokenizer.sep_token_id\n",
    "model.config.max_length = 64\n",
    "model.config.early_stopping = True\n",
    "model.config.no_repeat_ngram_size = 3\n",
    "model.config.length_penalty = 2.0\n",
    "model.config.num_beams = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I-_bwRqJFvDH"
   },
   "source": [
    "using steps "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_otg0vO8BOU3",
    "outputId": "bbd256b4-d0e9-4b9a-cddc-122739422257"
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "FP16 Mixed precision training with AMP or APEX (`--fp16`) and FP16 half precision evaluation (`--fp16_full_eval`) can only be used on CUDA devices.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [8]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Seq2SeqTrainer, Seq2SeqTrainingArguments\n\u001b[1;32m----> 3\u001b[0m training_args \u001b[38;5;241m=\u001b[39m \u001b[43mSeq2SeqTrainingArguments\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpredict_with_generate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mevaluation_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msteps\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mper_device_train_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mper_device_eval_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfp16\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogging_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m200\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_train_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m<string>:108\u001b[0m, in \u001b[0;36m__init__\u001b[1;34m(self, output_dir, overwrite_output_dir, do_train, do_eval, do_predict, evaluation_strategy, prediction_loss_only, per_device_train_batch_size, per_device_eval_batch_size, per_gpu_train_batch_size, per_gpu_eval_batch_size, gradient_accumulation_steps, eval_accumulation_steps, eval_delay, learning_rate, weight_decay, adam_beta1, adam_beta2, adam_epsilon, max_grad_norm, num_train_epochs, max_steps, lr_scheduler_type, warmup_ratio, warmup_steps, log_level, log_level_replica, log_on_each_node, logging_dir, logging_strategy, logging_first_step, logging_steps, logging_nan_inf_filter, save_strategy, save_steps, save_total_limit, save_on_each_node, no_cuda, use_mps_device, seed, data_seed, jit_mode_eval, use_ipex, bf16, fp16, fp16_opt_level, half_precision_backend, bf16_full_eval, fp16_full_eval, tf32, local_rank, xpu_backend, tpu_num_cores, tpu_metrics_debug, debug, dataloader_drop_last, eval_steps, dataloader_num_workers, past_index, run_name, disable_tqdm, remove_unused_columns, label_names, load_best_model_at_end, metric_for_best_model, greater_is_better, ignore_data_skip, sharded_ddp, fsdp, fsdp_min_num_params, fsdp_transformer_layer_cls_to_wrap, deepspeed, label_smoothing_factor, optim, adafactor, group_by_length, length_column_name, report_to, ddp_find_unused_parameters, ddp_bucket_cap_mb, dataloader_pin_memory, skip_memory_metrics, use_legacy_prediction_loop, push_to_hub, resume_from_checkpoint, hub_model_id, hub_strategy, hub_token, hub_private_repo, gradient_checkpointing, include_inputs_for_metrics, fp16_backend, push_to_hub_model_id, push_to_hub_organization, push_to_hub_token, mp_parameters, auto_find_batch_size, full_determinism, torchdynamo, ray_scope, ddp_timeout, sortish_sampler, predict_with_generate, generation_max_length, generation_num_beams)\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\training_args.py:1119\u001b[0m, in \u001b[0;36mTrainingArguments.__post_init__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1110\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptim \u001b[38;5;241m=\u001b[39m OptimizerNames\u001b[38;5;241m.\u001b[39mADAFACTOR\n\u001b[0;32m   1112\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   1113\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframework \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1114\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m is_torch_available()\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1117\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfp16 \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfp16_full_eval)\n\u001b[0;32m   1118\u001b[0m ):\n\u001b[1;32m-> 1119\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1120\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFP16 Mixed precision training with AMP or APEX (`--fp16`) and FP16 half precision evaluation\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1121\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m (`--fp16_full_eval`) can only be used on CUDA devices.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1122\u001b[0m     )\n\u001b[0;32m   1124\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   1125\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframework \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1126\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m is_torch_available()\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1130\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbf16 \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbf16_full_eval)\n\u001b[0;32m   1131\u001b[0m ):\n\u001b[0;32m   1132\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1133\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBF16 Mixed precision training with AMP (`--bf16`) and BF16 half precision evaluation\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1134\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m (`--bf16_full_eval`) can only be used on CUDA or CPU devices.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1135\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: FP16 Mixed precision training with AMP or APEX (`--fp16`) and FP16 half precision evaluation (`--fp16_full_eval`) can only be used on CUDA devices."
     ]
    }
   ],
   "source": [
    "from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    predict_with_generate=True,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    fp16=True, #uncomment this for CPU \n",
    "    output_dir=\"./\",\n",
    "    logging_steps=2,\n",
    "    save_steps=1000,\n",
    "    eval_steps=200,\n",
    "    num_train_epochs=10,  \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xz9lc08aF7pr"
   },
   "source": [
    "evaluating using epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1-AHdsYMFe__",
    "outputId": "3ffb1424-7622-4f51-a84e-9cc4f24b7349"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    }
   ],
   "source": [
    "training_args = Seq2SeqTrainingArguments(\n",
    "   evaluation_strategy = \"epoch\",\n",
    "   learning_rate=2e-4,\n",
    "   per_device_train_batch_size=32,\n",
    "   per_device_eval_batch_size=32,\n",
    "   weight_decay=0.01,\n",
    "   save_total_limit=3,\n",
    "   num_train_epochs=100,\n",
    "   output_dir=\"./\",\n",
    "   predict_with_generate=True,  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting datasets\n",
      "  Using cached datasets-2.6.1-py3-none-any.whl (441 kB)\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.2.0 in c:\\users\\ncl_ncai\\anaconda3\\lib\\site-packages (from datasets) (0.10.1)\n",
      "Collecting pyarrow>=6.0.0\n",
      "  Downloading pyarrow-9.0.0-cp39-cp39-win_amd64.whl (19.6 MB)\n",
      "     ---------------------------------------- 19.6/19.6 MB 1.4 MB/s eta 0:00:00\n",
      "Requirement already satisfied: fsspec[http]>=2021.11.1 in c:\\users\\ncl_ncai\\anaconda3\\lib\\site-packages (from datasets) (2022.2.0)\n",
      "Collecting multiprocess\n",
      "  Downloading multiprocess-0.70.13-py39-none-any.whl (132 kB)\n",
      "     -------------------------------------- 132.3/132.3 kB 2.6 MB/s eta 0:00:00\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\ncl_ncai\\anaconda3\\lib\\site-packages (from datasets) (1.21.5)\n",
      "Requirement already satisfied: dill<0.3.6 in c:\\users\\ncl_ncai\\anaconda3\\lib\\site-packages (from datasets) (0.3.5.1)\n",
      "Collecting responses<0.19\n",
      "  Using cached responses-0.18.0-py3-none-any.whl (38 kB)\n",
      "Requirement already satisfied: pandas in c:\\users\\ncl_ncai\\anaconda3\\lib\\site-packages (from datasets) (1.4.2)\n",
      "Requirement already satisfied: requests>=2.19.0 in c:\\users\\ncl_ncai\\anaconda3\\lib\\site-packages (from datasets) (2.27.1)\n",
      "Note: you may need to restart the kernel to use updated packages.Requirement already satisfied: tqdm>=4.62.1 in c:\\users\\ncl_ncai\\anaconda3\\lib\\site-packages (from datasets) (4.64.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\ncl_ncai\\anaconda3\\lib\\site-packages (from datasets) (21.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\ncl_ncai\\anaconda3\\lib\\site-packages (from datasets) (6.0)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\ncl_ncai\\anaconda3\\lib\\site-packages (from datasets) (3.8.1)\n",
      "Collecting xxhash\n",
      "  Downloading xxhash-3.1.0-cp39-cp39-win_amd64.whl (30 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\ncl_ncai\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets) (3.6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\ncl_ncai\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets) (4.1.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.2.2 -> 22.3\n",
      "[notice] To update, run: C:\\Users\\NCL_NCAI\\anaconda3\\python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\ncl_ncai\\anaconda3\\lib\\site-packages (from packaging->datasets) (3.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\ncl_ncai\\anaconda3\\lib\\site-packages (from requests>=2.19.0->datasets) (1.26.9)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\ncl_ncai\\anaconda3\\lib\\site-packages (from requests>=2.19.0->datasets) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\ncl_ncai\\anaconda3\\lib\\site-packages (from requests>=2.19.0->datasets) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\ncl_ncai\\anaconda3\\lib\\site-packages (from requests>=2.19.0->datasets) (2021.10.8)\n",
      "Requirement already satisfied: colorama in c:\\users\\ncl_ncai\\anaconda3\\lib\\site-packages (from tqdm>=4.62.1->datasets) (0.4.4)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\ncl_ncai\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (21.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\ncl_ncai\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (1.2.0)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in c:\\users\\ncl_ncai\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (4.0.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\ncl_ncai\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (5.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\ncl_ncai\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (1.2.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\ncl_ncai\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (1.6.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\ncl_ncai\\anaconda3\\lib\\site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\ncl_ncai\\anaconda3\\lib\\site-packages (from pandas->datasets) (2021.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\ncl_ncai\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
      "Installing collected packages: xxhash, pyarrow, multiprocess, responses, datasets\n",
      "Successfully installed datasets-2.6.1 multiprocess-0.70.13 pyarrow-9.0.0 responses-0.18.0 xxhash-3.1.0\n"
     ]
    }
   ],
   "source": [
    "pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting jiwer\n",
      "  Using cached jiwer-2.5.1-py3-none-any.whl (15 kB)\n",
      "Collecting levenshtein==0.20.2\n",
      "  Downloading Levenshtein-0.20.2-cp39-cp39-win_amd64.whl (182 kB)\n",
      "     ------------------------------------ 182.5/182.5 kB 736.4 kB/s eta 0:00:00\n",
      "Collecting rapidfuzz<3.0.0,>=2.3.0\n",
      "  Downloading rapidfuzz-2.11.1-cp39-cp39-win_amd64.whl (993 kB)\n",
      "     -------------------------------------- 993.6/993.6 kB 3.1 MB/s eta 0:00:00\n",
      "Installing collected packages: rapidfuzz, levenshtein, jiwer\n",
      "Successfully installed jiwer-2.5.1 levenshtein-0.20.2 rapidfuzz-2.11.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.2.2 -> 22.3\n",
      "[notice] To update, run: C:\\Users\\NCL_NCAI\\anaconda3\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install jiwer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "-UNPv-KiBtvp"
   },
   "outputs": [],
   "source": [
    "from datasets import load_metric\n",
    "cer_metric = load_metric(\"cer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "vOYddNXnBxr2"
   },
   "outputs": [],
   "source": [
    "def compute_metrics(pred):\n",
    "    labels_ids = pred.label_ids\n",
    "    pred_ids = pred.predictions\n",
    "\n",
    "    pred_str = processor.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "    labels_ids[labels_ids == -100] = processor.tokenizer.pad_token_id\n",
    "    label_str = processor.batch_decode(labels_ids, skip_special_tokens=True)\n",
    "\n",
    "    cer = cer_metric.compute(predictions=pred_str, references=label_str)\n",
    "\n",
    "    return {\"cer\": cer}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 373
    },
    "id": "8k_Ufd6HB1Px",
    "outputId": "8abac32d-8f52-44a6-bf32-74a647e4e471"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 7044\n",
      "  Num Epochs = 100\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 22100\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='95' max='22100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [   95/22100 5:24:40 < 1280:24:13, 0.00 it/s, Epoch 0.43/100]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import default_data_collator\n",
    "\n",
    "# instantiate trainer\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    tokenizer=processor.feature_extractor,\n",
    "    args=training_args,\n",
    "    compute_metrics=compute_metrics,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    data_collator=default_data_collator\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "D3J3XGY6GfkD",
    "outputId": "ff8c226a-e366-46a1-c992-ec1b9d655833"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./trainer\n",
      "Configuration saved in ./trainer/config.json\n",
      "Model weights saved in ./trainer/pytorch_model.bin\n",
      "Feature extractor saved in ./trainer/preprocessor_config.json\n"
     ]
    }
   ],
   "source": [
    "trainer.save_model('./trainer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Jzfohl8wG2J2"
   },
   "outputs": [],
   "source": [
    "model = VisionEncoderDecoderModel.from_pretrained(\"./trainer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "id": "okXisArpHLzQ"
   },
   "outputs": [],
   "source": [
    "image = Image.open('/content/40.png').convert(\"RGB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 57
    },
    "id": "VQnwWb5LPngA",
    "outputId": "8a57aa99-dbd4-4d85-e397-13c72bf7b41c"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABOUAAAArCAIAAAC4kKsHAAADQUlEQVR4nO3dzW6jMBSA0TLq+78ys6iEUCBgsA33wjmrjNq6Tlb9xj8M4zj+AAAAlBmGQURwjX93TwAAAEhjGIZ+I/cbnKR+754AAADwalOmWrblg/VVAADgHvM11aOxajH2DayvAgAARRom4sdQYpVVehUAALhOZalOI9g8/AZ6FQAAuEJ9qS4H4dn0KgAA0N28M08vjbqZ6W3ctwQAANQqXPYcx3E7NTfGEasvZH0VAACoshurhSdOv41TuTa7HFbxZmF9FQAAiKt5rJKIXgUAAIrU3JBUf7uSWH0h+4EBAID7LbO28sDq8sc9CCcdvQoAAITTPFZPD8WN7AcGAABiaXUVsEDNTq8CAACBlMfq6eOpzrVmoVcBACCfYRj+omt68TA1sbr9gTzy43oqvQoAAMlMxfW89Jq/td13V9K0q4PYJ5yFXgUAgEyaN2rk6G0yt/kgkd8sS3oVAABymFYLP5YHa1YL4/fb6gxrpm1xNRG9CgAA0c33tf7l1jiO04vKwa/ptyvDePmOprO+l82BJvQqAADE9XECs+HK6vVOXw3VtjOfekPVI/3ePQEAAGDdRqk2H7/coZmc+BXT+Ns/uz2Nv6/q0uysrwIAQETzDcC51lEnNbE6f/3x9suHbbhrmlvoVQAACGe6VylUaB3qz4APkvkWwISlVwEAIJaku1h3HxtTUom9n9YT7b8A2Ob8KgAAdFQYYKsR1fv86iGrj9JZ/Z65E9NeFua3b7v9M6E3vQoAAIF869tWC4/nGu+Cm5l6KGlsIrMfGAAAOiqJpcuCancVd9XHo19rflfD79+VdFs1c9ZXAQCgr3mJ7e6YHcexR2idrsGSyXSa80/B9uP4+6ipYc83AACE0yT/dv/U3zgpeuIkapNKLBzk28ybHKAlDr0KAAAvdehBpl1nclTNLVYk4vwqAAC8VGHOJa2+pNNmTq8CAEAmF18jFLP6dmcVc9ocpVcBACCN5rG63XWRq29jbpGnzSHOrwIAQBrfbsRtMvL8n1kyIem0KaRXAQAgh7828wc872E/MAAAABHpVQAASMPiKq9iPzAAACTgoCYvZH0VAACAiPQqAAAkYEGVF9KrAACQjHblJf4DAu5ojTLwPZ8AAAAASUVORK5CYII=",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=1253x43 at 0x7FE273DE97D0>"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A5MR7dxhHR7W",
    "outputId": "28bdad7a-c39e-4859-e71f-9e749b5d7122"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 384, 384])\n"
     ]
    }
   ],
   "source": [
    "pixel_values = processor.feature_extractor(image, return_tensors=\"pt\").pixel_values \n",
    "print(pixel_values.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2e4ruhWiHp4-",
    "outputId": "eb716839-f2eb-4701-90c3-39c657508964"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ہی کا کمال ہے\n"
     ]
    }
   ],
   "source": [
    "generated_ids = model.generate(pixel_values)\n",
    "generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "print(generated_text)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "making_processor.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
